{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Pipeline Deployment</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have trained our models for feature engineering and binary classification, we can choose to deploy them in pipeline using Amazon SageMaker Inference Pipelines.\n",
    "\n",
    "This notebook demonstrates how to create a pipeline with the SKLearn model for feature engineering and the Amazon SageMaker Linear Learner model for binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sagemaker.session.Session object at 0x7f8782c90588>\n",
      "eu-west-1\n",
      "arn:aws:iam::825935527263:role/gianpo-path/SageMaker-Notebook-Role\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "sagemaker_session = sagemaker.Session(boto3.Session())\n",
    "\n",
    "print(sagemaker_session)\n",
    "\n",
    "print(region)\n",
    "print(role)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to create SageMaker Model objects, which represent the objects that binds together the artifacts of training (model artifacts as S3 objects) and the Docker container used for inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to specify the paths to our models in S3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_preprocessor_path = 's3://gianpo-predictive-maintenance/output/predmain-expprep-sklearn-2019-05-02-18-50-10-249/output/model.tar.gz'\n",
    "linear_learner_model_path = 's3://gianpo-predictive-maintenance/output/predmain-training-ll-2019-05-04-14-18-50-815/output/model.tar.gz'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need to find the linear learner Docker containers path in ECR:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "438346466558.dkr.ecr.eu-west-1.amazonaws.com/linear-learner:latest\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "linear_learner_container = get_image_uri(boto3.Session().region_name, 'linear-learner', repo_version=\"latest\")\n",
    "print(linear_learner_container)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to create our models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model import Model\n",
    "from sagemaker.sklearn.model import SKLearnModel\n",
    "\n",
    "sklearn_preprocessor = SKLearnModel(sklearn_preprocessor_path, role, '1-predmain-expprep-sklearn-script.py', sagemaker_session=sagemaker_session)\n",
    "linear_learner_model = Model(linear_learner_model_path, linear_learner_container, sagemaker_session=sagemaker_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have models ready, we can deploy them in a pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: predmain-sk-ll-pipeline\n",
      "WARNING:sagemaker:Using already existing model: predmain-sk-ll-pipeline\n",
      "INFO:sagemaker:Creating endpoint with name predmain-sk-ll-endpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------!"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker.pipeline import PipelineModel\n",
    "\n",
    "pipeline_model = PipelineModel(\n",
    "    name='predmain-sk-ll-pipeline', \n",
    "    role=role, \n",
    "    models=[\n",
    "        sklearn_preprocessor, \n",
    "        linear_learner_model],\n",
    "    sagemaker_session=sagemaker_session)\n",
    "\n",
    "pipeline_model.deploy(initial_instance_count=1, \n",
    "                      instance_type='ml.c5.xlarge', \n",
    "                      endpoint_name='predmain-sk-ll-endpoint')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Getting inferences</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can try invoking our pipeline of models and try getting some inferences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'{\"predictions\": [{\"score\": 0.6406679749488831, \"predicted_label\": 1.0}]}'\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.predictor import json_serializer, csv_serializer, json_deserializer, RealTimePredictor\n",
    "from sagemaker.content_types import CONTENT_TYPE_CSV, CONTENT_TYPE_JSON\n",
    "\n",
    "predictor = RealTimePredictor(\n",
    "    endpoint='predmain-sk-ll-endpoint',\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    serializer=csv_serializer,\n",
    "    content_type=CONTENT_TYPE_CSV,\n",
    "    accept=CONTENT_TYPE_JSON)\n",
    "\n",
    "payload = 'TID008,VAWT,64,80,46.0,21,55,55,7,34,NE'\n",
    "print(predictor.predict(payload))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Cleanup</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once finished testing the endpoint and getting some inferences, we can delete the endopoint to avoid incurring in unexpected charges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting endpoint configuration with name: predmain-sk-ll-endpoint\n",
      "INFO:sagemaker:Deleting endpoint with name: predmain-sk-ll-endpoint\n"
     ]
    }
   ],
   "source": [
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
